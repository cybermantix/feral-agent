<?php

namespace Feral\Agent\Process\NodeCode;

use Feral\Core\Process\Attributes\CatalogNodeDecorator;
use Feral\Core\Process\Attributes\IntConfigurationDescription;
use Feral\Core\Process\Attributes\OkResultDescription;
use Feral\Core\Process\Attributes\ResultDescription;
use Feral\Core\Process\Attributes\StringConfigurationDescription;
use Feral\Core\Process\Configuration\ConfigurationManager;
use Feral\Core\Process\Context\ContextInterface;
use Feral\Core\Process\Exception\MissingConfigurationValueException;
use Feral\Core\Process\NodeCode\Category\NodeCodeCategoryInterface;
use Feral\Core\Process\NodeCode\NodeCodeInterface;
use Feral\Core\Process\NodeCode\Traits\ConfigurationTrait;
use Feral\Core\Process\NodeCode\Traits\ConfigurationValueTrait;
use Feral\Core\Process\NodeCode\Traits\ContextMutationTrait;
use Feral\Core\Process\NodeCode\Traits\ContextValueTrait;
use Feral\Core\Process\NodeCode\Traits\NodeCodeMetaTrait;
use Feral\Core\Process\NodeCode\Traits\ResultsTrait;
use Feral\Core\Process\Result\ResultInterface;
use Feral\Core\Utility\Filter\Comparator\Exception\UnknownComparatorException;
use Feral\Core\Utility\Search\DataPathReader;
use Feral\Core\Utility\Search\DataPathReaderInterface;
use Feral\Core\Utility\Search\DataPathWriter;
use Symfony\Component\HttpClient\HttpClient;
use Symfony\Component\HttpClient\Psr18Client;

/**
 * Roles Available in OpenAI API Requests
 * System -This role is used to set the behavior or context for the
 * assistant. It defines how the assistant should respond to user
 * inputs. For example, a system message might state, "You are a helpful assistant."
 *
 * User - The user role represents the input or queries made by the
 * user. It captures what the user wants to ask or convey to the assistant.
 * For instance, a user message could be "What is the weather like today?"
 *
 * Assistant - This role is used for the responses generated by the model
 * based on the userâ€™s input and the system's instructions. The assistant
 * role reflects what the model outputs in response to user messages.
 */
#[StringConfigurationDescription(
    key: self::BASE_PATH,
    name: 'Base Path',
    description: 'The base path to make the request.',
    isOptional: true
)]
#[StringConfigurationDescription(
    key: self::BEARER_TOKEN_VARIABLE,
    name: 'Bearer Token',
    description: 'Which environment variable does the bearer token live.',
    isOptional: false
)]
#[StringConfigurationDescription(
    key: self::MODEL,
    name: 'Model',
    description: 'The model used to handel the request.',
    isOptional: true
)]
#[IntConfigurationDescription(
    key: self::MAX_TOKENS,
    name: 'Max Tokens',
    description: 'The maximum number of tokens that can be used',
    isOptional: true,
    default: 6500
)]
#[StringConfigurationDescription(
    key: self::OUTPUT_CONTEXT_PATH,
    name: 'Output Context Path',
    description: 'The context location where the results will be stored.',
    isOptional: true,
    default: self::DEFAULT_OUTPUT_CONTEXT_PATH
)]
#[StringConfigurationDescription(
    key: self::SYSTEM_INPUT_CONTEXT_PATH,
    name: 'System Input Context Path',
    description: 'The context path where the system messages are stored.',
    isOptional: true,
    default: self::DEFAULT_SYSTEM_INPUT_CONTEXT_PATH
)]
#[StringConfigurationDescription(
    key: self::USER_INPUT_CONTEXT_PATH,
    name: 'User Input Context Path',
    description: 'The context path where the user messages are stored.',
    isOptional: true,
    default: self::DEFAULT_USER_INPUT_CONTEXT_PATH
)]
#[StringConfigurationDescription(
    key: self::ASSISTANT_INPUT_CONTEXT_PATH,
    name: 'Assistant Input Context Path',
    description: 'The context path where the assistant messages are stored.',
    isOptional: true,
    default: self::DEFAULT_ASSISTANT_INPUT_CONTEXT_PATH
)]
#[OkResultDescription(description: 'The call to OpenAI was successful.')]
#[ResultDescription(result: 'Error', description: 'The call to OpenAI did not return a message.')]
#[CatalogNodeDecorator(
    key:'open_ai_4o',
    name: 'OpenAI GPT-4o',
    group: 'GenAI',
    description: 'Send a query to OpenAI using the GTP 4o model.',
    configuration: [
        self::BASE_PATH => 'https://api.openai.com/v1/',
        self::MODEL => 'gpt-4o',
        self::BEARER_TOKEN_VARIABLE => 'OPENAPI_KEY',
    ]
)]
#[CatalogNodeDecorator(
    key:'open_ai_4o_mini',
    name: 'OpenAI GPT-4o-mini',
    group: 'GenAI',
    description: 'Send a query to OpenAI using the GTP 4o mini model.',
    configuration: [
        self::BASE_PATH => 'https://api.openai.com/v1/',
        self::MODEL => 'gpt-4o-mini',
        self::BEARER_TOKEN_VARIABLE => 'OPENAPI_KEY',
    ]
)]
#[CatalogNodeDecorator(
    key:'open_ai_4_turbo',
    name: 'OpenAI GPT-4 Turbo',
    group: 'GenAI',
    description: 'Send a query to OpenAI using the GTP 4 turbo model.',
    configuration: [
        self::BASE_PATH => 'https://api.openai.com/v1/',
        self::MODEL => 'gpt-4o-mini',
        self::BEARER_TOKEN_VARIABLE => 'OPENAPI_KEY',
    ]
)]
#[CatalogNodeDecorator(
    key:'perplexity_small',
    name: 'Perplexity Large',
    group: 'GenAI',
    description: 'Send a query to Perplexity using the large model.',
    configuration: [
        self::BASE_PATH => 'https://api.perplexity.ai',
        self::MODEL => 'llama-3.1-sonar-large-128k-online',
        self::BEARER_TOKEN_VARIABLE => 'PERPLEXITY_KEY',
    ]
)]
#[CatalogNodeDecorator(
    key:'perplexity_large',
    name: 'Perplexity Large',
    group: 'GenAI',
    description: 'Send a query to Perplexity using the large model.',
    configuration: [
        self::BASE_PATH => 'https://api.perplexity.ai',
        self::MODEL => 'llama-3.1-sonar-large-128k-online',
        self::BEARER_TOKEN_VARIABLE => 'PERPLEXITY_KEY',
    ]
)]
#[CatalogNodeDecorator(
    key:'perplexity_huge',
    name: 'Perplexity Huge',
    group: 'GenAI',
    description: 'Send a query to Perplexity using the huge model.',
    configuration: [
        self::BASE_PATH => 'https://api.perplexity.ai',
        self::MODEL => 'llama-3.1-sonar-huge-128k-online',
        self::BEARER_TOKEN_VARIABLE => 'PERPLEXITY_KEY',
    ]
)]
class OpenAiNodeCode implements NodeCodeInterface
{
    use NodeCodeMetaTrait,
        ResultsTrait,
        ConfigurationTrait,
        ConfigurationValueTrait,
        ContextValueTrait,
        ContextMutationTrait;

    public function __construct(
        DataPathReaderInterface $dataPathReader = new DataPathReader(),
        DataPathWriter $dataPathWriter = new DataPathWriter(),
        ConfigurationManager $configurationManager = new ConfigurationManager()
    ) {
        $this->setMeta(
            self::KEY,
            self::NAME,
            self::DESCRIPTION,
            NodeCodeCategoryInterface::DATA
        )
            ->setConfigurationManager($configurationManager)
            ->setDataPathWriter($dataPathWriter)
            ->setDataPathReader($dataPathReader);
    }

    const KEY = 'open_ai';

    const NAME = 'OpenAI';

    const DESCRIPTION = 'Call the OpenAI API using data in the context and storing data returned.';

    const ROLE = 'role';
    const CONTENT = 'content';
    const ROLE_SYSTEM = 'system';
    const ROLE_USER = 'user';
    const ROLE_ASSISTANT = 'assistant';
    public const DEFAULT_SERVER_URL = '';
    public const DEFAULT_OUTPUT_CONTEXT_PATH = 'result';
    public const DEFAULT_SYSTEM_INPUT_CONTEXT_PATH = '_system_prompts';
    public const DEFAULT_USER_INPUT_CONTEXT_PATH = '_user_prompts';
    public const DEFAULT_ASSISTANT_INPUT_CONTEXT_PATH = '_assistant_prompts';
    public const BASE_PATH = 'base_path';
    public const BEARER_TOKEN_VARIABLE = 'bearer_token_variable';
    public const MODEL = 'model';
    public const MAX_TOKENS = 'max_tokens';
    public const OUTPUT_CONTEXT_PATH = 'output_context_path';
    public const SYSTEM_INPUT_CONTEXT_PATH = 'system_context_path';
    public const USER_INPUT_CONTEXT_PATH = 'user_context_path';
    public const ASSISTANT_INPUT_CONTEXT_PATH = 'assistant_context_path';

    /**
     * @inheritDoc
     * @throws     MissingConfigurationValueException|UnknownComparatorException
     * @throws     Exception
     */
    public function process(ContextInterface $context): ResultInterface
    {
        $basePath = $this->getRequiredConfigurationValue(self::BASE_PATH, self::DEFAULT_SERVER_URL);
        $bearerTokenVariable = $this->getRequiredStringConfigurationValue(self::BEARER_TOKEN_VARIABLE);
        $model = $this->getRequiredConfigurationValue(self::MODEL);
        $maxTokens = $this->getRequiredIntConfigurationValue(self::MAX_TOKENS);
        $systemPath = $this->getConfigurationValue(self::SYSTEM_INPUT_CONTEXT_PATH, self::DEFAULT_SYSTEM_INPUT_CONTEXT_PATH);
        $userPath = $this->getConfigurationValue(self::USER_INPUT_CONTEXT_PATH, self::DEFAULT_USER_INPUT_CONTEXT_PATH);
        $assistantPath = $this->getConfigurationValue(self::ASSISTANT_INPUT_CONTEXT_PATH, self::DEFAULT_ASSISTANT_INPUT_CONTEXT_PATH);
        $resultPath = $this->getRequiredConfigurationValue(self::OUTPUT_CONTEXT_PATH, self::DEFAULT_OUTPUT_CONTEXT_PATH);
        $systemPrompts = (array) $this->getValueFromContext($systemPath, $context);
        $userPrompts = (array) $this->getValueFromContext($userPath, $context);
        $assistantPrompts = (array) $this->getValueFromContext($assistantPath, $context);

        // THE BEARER TOKEN SHOULD BE IN ENV.LOCAL
        $bearerToken = $_ENV[$bearerTokenVariable];

        $messages = [];
        $systemMessages = [];
        $userMessages = [];
        $assistantMessages = [];
        foreach ($systemPrompts as $prompt) {
            $systemMessages[] = [
                self::ROLE => self::ROLE_SYSTEM,
                self::CONTENT => $prompt];
        }
        foreach ($userPrompts as $prompt) {
            $userMessages[] = [
                self::ROLE => self::ROLE_USER,
                self::CONTENT => $prompt];
        }
        foreach ($assistantPrompts as $prompt) {
            $assistantMessages[] = [
                self::ROLE => self::ROLE_ASSISTANT,
                self::CONTENT => $prompt];
        }

        $maxLength = max(count($systemMessages), count($userMessages), count($assistantMessages));

        for ($i = 0; $i < $maxLength; $i++) {
            if (isset($systemMessages[$i])) {
                $messages[] = $systemMessages[$i];
            }
            if (isset($userMessages[$i])) {
                $messages[] = $userMessages[$i];
            }
            if (isset($assistantMessages[$i])) {
                $messages[] = $assistantMessages[$i];
            }
        }

        if (empty($messages)) {
            throw new \Exception('One to many messages are required.');
        }

        $httpClient = HttpClient::create([
            'timeout' => 120, // Timeout in seconds
        ]);

        $psr18Client = new Psr18Client($httpClient);

        $client = \OpenAI::factory()
            ->withHttpClient($psr18Client)
            ->withApiKey($bearerToken)
            ->withBaseUri($basePath)
            ->make();

        $result = $client->chat()->create([
            'model' => $model,
            'messages' => $messages,
            'max_tokens' => $maxTokens,
        ]);

        if (!empty($result->choices[0]->message->content)) {
            $this->setValueInContext($resultPath, $result, $context);
            return $this->result(
                ResultInterface::OK,
                'Called GenAI system with model "%s" and base path "%s" with %u messages. Tokens: prompt %u, completion %u, total %u',
                [$model, $basePath, count($messages), $result->usage->promptTokens, $result->usage->completionTokens, $result->usage->totalTokens]
            );
        } else {
            return $this->result(
                ResultInterface::ERROR,
                'No result messages were returned when calling model "%s" and base path "%s" with %u messages. Tokens: prompt %u, completion %u, total %u',
                [$model, $basePath, count($messages), $result->usage->promptTokens, $result->usage->completionTokens, $result->usage->totalTokens]
            );
        }
    }
}